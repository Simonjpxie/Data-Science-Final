{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505bd44a",
   "metadata": {},
   "source": [
    "# ECON 5181 Assignment 3\n",
    "\n",
    "Due: May 2, 2024, noon. Submission on Blackboard. \n",
    "\n",
    "DON'T forget your VeriGuide report and honesty declaration! \n",
    "\n",
    "*The copyright of this content, produced by staff members/ teachers of The Chinese University of Hong Kong (CUHK), belongs to CUHK.  Students shall not distribute, share, copy or upload the content to a third party.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59cb29",
   "metadata": {},
   "source": [
    "### Q1. Foreign Exchange\n",
    "\n",
    "What are the latent factors of international currency pricing? And how do these factors move against US equities? In this exercise, we are going to investigate the underlying factors in currency exchange rates and regress the S&P 500 onto this information.\n",
    "\n",
    "* FX data is in `FXmonthly.csv`.\n",
    "\n",
    "* SP500 returns are in `sp500csv`.\n",
    "\n",
    "* Currency codes are in `currency_codes.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fffc06",
   "metadata": {},
   "source": [
    "First of all, some data preparation...\n",
    "### Data preparation\n",
    "\n",
    "These numbers are amount in local currency to buy 1 USD. The higher the weaker that currency is against US dollars. Note that these were options, not the true historic prices.\n",
    "\n",
    "We translate the prices to returns via"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6817d73",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fx <- read.csv(\"./fx/FXmonthly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35de4ecd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>exalus</th><th scope=col>exbzus</th><th scope=col>excaus</th><th scope=col>exchus</th><th scope=col>exdnus</th><th scope=col>exhkus</th><th scope=col>exinus</th><th scope=col>exjpus</th><th scope=col>exkous</th><th scope=col>exmaus</th><th scope=col>...</th><th scope=col>exsius</th><th scope=col>exsfus</th><th scope=col>exslus</th><th scope=col>exsdus</th><th scope=col>exszus</th><th scope=col>extaus</th><th scope=col>exthus</th><th scope=col>exukus</th><th scope=col>exvzus</th><th scope=col>exeuus</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>FEB2001</th><td> 0.040084388 </td><td>0.02550994   </td><td> 0.0122405535</td><td>-6.040398e-05</td><td> 0.01851084  </td><td> 1.282084e-05</td><td>-0.001072731 </td><td>-0.003771321 </td><td>-0.015542616 </td><td>0            </td><td>...          </td><td>0.003164557  </td><td> 0.005502275 </td><td> 0.0151806415</td><td> 0.027478664 </td><td> 0.02336707  </td><td>-0.010497965 </td><td>-0.0112169459</td><td> 0.017287234 </td><td>0.00000000   </td><td> 0.018563660 </td></tr>\n",
       "\t<tr><th scope=row>MAR2001</th><td> 0.061012064 </td><td>0.04461615   </td><td> 0.0243822292</td><td> 4.832610e-05</td><td> 0.01388358  </td><td> 0.000000e+00</td><td> 0.001932990 </td><td> 0.045427170 </td><td> 0.030777827 </td><td>0            </td><td>...          </td><td>0.017034700  </td><td> 0.009793643 </td><td>-0.0161356959</td><td> 0.030743042 </td><td> 0.01330457  </td><td> 0.009031859 </td><td> 0.0310090238</td><td> 0.005519245 </td><td>0.01428571   </td><td> 0.013438881 </td></tr>\n",
       "\t<tr><th scope=row>APR2001</th><td> 0.002968255 </td><td>0.04671916   </td><td>-0.0005774042</td><td>-4.832377e-05</td><td> 0.01736614  </td><td>-7.692406e-05</td><td> 0.003001072 </td><td> 0.018599292 </td><td> 0.028147529 </td><td>0            </td><td>...          </td><td>0.021768554  </td><td> 0.022828564 </td><td> 0.0288697072</td><td> 0.015112022 </td><td> 0.01318902  </td><td> 0.009778677 </td><td> 0.0342366100</td><td> 0.006788964 </td><td>0.00000000   </td><td> 0.017620345 </td></tr>\n",
       "\t<tr><th scope=row>MAY2001</th><td>-0.035212681 </td><td>0.04522659   </td><td>-0.0107202465</td><td>-1.208153e-05</td><td> 0.01911376  </td><td> 7.692998e-05</td><td> 0.003419534 </td><td>-0.016159005 </td><td>-0.021735856 </td><td>0            </td><td>...          </td><td>0.001269456  </td><td>-0.012304569 </td><td> 0.0299642877</td><td> 0.014485226 </td><td> 0.02317436  </td><td> 0.007953614 </td><td> 0.0006814085</td><td> 0.005738881 </td><td>0.00000000   </td><td> 0.019725098 </td></tr>\n",
       "\t<tr><th scope=row>JUN2001</th><td> 0.003691380 </td><td>0.03759923   </td><td>-0.0107715268</td><td> 0.000000e+00</td><td> 0.02511260  </td><td>-2.564135e-05</td><td> 0.001916933 </td><td> 0.004763078 </td><td>-0.002964047 </td><td>0            </td><td>...          </td><td>0.001598589  </td><td> 0.010101643 </td><td>-0.0052505284</td><td> 0.042670969 </td><td> 0.01871292  </td><td> 0.033882480 </td><td>-0.0057550796</td><td> 0.017546362 </td><td>0.01408451   </td><td> 0.026083151 </td></tr>\n",
       "\t<tr><th scope=row>JUL2001</th><td> 0.017871018 </td><td>0.03964184   </td><td> 0.0041325025</td><td>-1.208167e-05</td><td>-0.01092715  </td><td> 2.564201e-05</td><td> 0.002976190 </td><td> 0.017572538 </td><td> 0.007868422 </td><td>0            </td><td>...          </td><td>0.003467254  </td><td> 0.018599169 </td><td>-0.0006307333</td><td>-0.003029741 </td><td>-0.01601703  </td><td> 0.014361454 </td><td> 0.0083511919</td><td>-0.009112575 </td><td>0.00000000   </td><td>-0.009809776 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllllllllllllllllllll}\n",
       "  & exalus & exbzus & excaus & exchus & exdnus & exhkus & exinus & exjpus & exkous & exmaus & ... & exsius & exsfus & exslus & exsdus & exszus & extaus & exthus & exukus & exvzus & exeuus\\\\\n",
       "\\hline\n",
       "\tFEB2001 &  0.040084388  & 0.02550994    &  0.0122405535 & -6.040398e-05 &  0.01851084   &  1.282084e-05 & -0.001072731  & -0.003771321  & -0.015542616  & 0             & ...           & 0.003164557   &  0.005502275  &  0.0151806415 &  0.027478664  &  0.02336707   & -0.010497965  & -0.0112169459 &  0.017287234  & 0.00000000    &  0.018563660 \\\\\n",
       "\tMAR2001 &  0.061012064  & 0.04461615    &  0.0243822292 &  4.832610e-05 &  0.01388358   &  0.000000e+00 &  0.001932990  &  0.045427170  &  0.030777827  & 0             & ...           & 0.017034700   &  0.009793643  & -0.0161356959 &  0.030743042  &  0.01330457   &  0.009031859  &  0.0310090238 &  0.005519245  & 0.01428571    &  0.013438881 \\\\\n",
       "\tAPR2001 &  0.002968255  & 0.04671916    & -0.0005774042 & -4.832377e-05 &  0.01736614   & -7.692406e-05 &  0.003001072  &  0.018599292  &  0.028147529  & 0             & ...           & 0.021768554   &  0.022828564  &  0.0288697072 &  0.015112022  &  0.01318902   &  0.009778677  &  0.0342366100 &  0.006788964  & 0.00000000    &  0.017620345 \\\\\n",
       "\tMAY2001 & -0.035212681  & 0.04522659    & -0.0107202465 & -1.208153e-05 &  0.01911376   &  7.692998e-05 &  0.003419534  & -0.016159005  & -0.021735856  & 0             & ...           & 0.001269456   & -0.012304569  &  0.0299642877 &  0.014485226  &  0.02317436   &  0.007953614  &  0.0006814085 &  0.005738881  & 0.00000000    &  0.019725098 \\\\\n",
       "\tJUN2001 &  0.003691380  & 0.03759923    & -0.0107715268 &  0.000000e+00 &  0.02511260   & -2.564135e-05 &  0.001916933  &  0.004763078  & -0.002964047  & 0             & ...           & 0.001598589   &  0.010101643  & -0.0052505284 &  0.042670969  &  0.01871292   &  0.033882480  & -0.0057550796 &  0.017546362  & 0.01408451    &  0.026083151 \\\\\n",
       "\tJUL2001 &  0.017871018  & 0.03964184    &  0.0041325025 & -1.208167e-05 & -0.01092715   &  2.564201e-05 &  0.002976190  &  0.017572538  &  0.007868422  & 0             & ...           & 0.003467254   &  0.018599169  & -0.0006307333 & -0.003029741  & -0.01601703   &  0.014361454  &  0.0083511919 & -0.009112575  & 0.00000000    & -0.009809776 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | exalus | exbzus | excaus | exchus | exdnus | exhkus | exinus | exjpus | exkous | exmaus | ... | exsius | exsfus | exslus | exsdus | exszus | extaus | exthus | exukus | exvzus | exeuus |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| FEB2001 |  0.040084388  | 0.02550994    |  0.0122405535 | -6.040398e-05 |  0.01851084   |  1.282084e-05 | -0.001072731  | -0.003771321  | -0.015542616  | 0             | ...           | 0.003164557   |  0.005502275  |  0.0151806415 |  0.027478664  |  0.02336707   | -0.010497965  | -0.0112169459 |  0.017287234  | 0.00000000    |  0.018563660  |\n",
       "| MAR2001 |  0.061012064  | 0.04461615    |  0.0243822292 |  4.832610e-05 |  0.01388358   |  0.000000e+00 |  0.001932990  |  0.045427170  |  0.030777827  | 0             | ...           | 0.017034700   |  0.009793643  | -0.0161356959 |  0.030743042  |  0.01330457   |  0.009031859  |  0.0310090238 |  0.005519245  | 0.01428571    |  0.013438881  |\n",
       "| APR2001 |  0.002968255  | 0.04671916    | -0.0005774042 | -4.832377e-05 |  0.01736614   | -7.692406e-05 |  0.003001072  |  0.018599292  |  0.028147529  | 0             | ...           | 0.021768554   |  0.022828564  |  0.0288697072 |  0.015112022  |  0.01318902   |  0.009778677  |  0.0342366100 |  0.006788964  | 0.00000000    |  0.017620345  |\n",
       "| MAY2001 | -0.035212681  | 0.04522659    | -0.0107202465 | -1.208153e-05 |  0.01911376   |  7.692998e-05 |  0.003419534  | -0.016159005  | -0.021735856  | 0             | ...           | 0.001269456   | -0.012304569  |  0.0299642877 |  0.014485226  |  0.02317436   |  0.007953614  |  0.0006814085 |  0.005738881  | 0.00000000    |  0.019725098  |\n",
       "| JUN2001 |  0.003691380  | 0.03759923    | -0.0107715268 |  0.000000e+00 |  0.02511260   | -2.564135e-05 |  0.001916933  |  0.004763078  | -0.002964047  | 0             | ...           | 0.001598589   |  0.010101643  | -0.0052505284 |  0.042670969  |  0.01871292   |  0.033882480  | -0.0057550796 |  0.017546362  | 0.01408451    |  0.026083151  |\n",
       "| JUL2001 |  0.017871018  | 0.03964184    |  0.0041325025 | -1.208167e-05 | -0.01092715   |  2.564201e-05 |  0.002976190  |  0.017572538  |  0.007868422  | 0             | ...           | 0.003467254   |  0.018599169  | -0.0006307333 | -0.003029741  | -0.01601703   |  0.014361454  |  0.0083511919 | -0.009112575  | 0.00000000    | -0.009809776  |\n",
       "\n"
      ],
      "text/plain": [
       "        exalus       exbzus     excaus        exchus        exdnus     \n",
       "FEB2001  0.040084388 0.02550994  0.0122405535 -6.040398e-05  0.01851084\n",
       "MAR2001  0.061012064 0.04461615  0.0243822292  4.832610e-05  0.01388358\n",
       "APR2001  0.002968255 0.04671916 -0.0005774042 -4.832377e-05  0.01736614\n",
       "MAY2001 -0.035212681 0.04522659 -0.0107202465 -1.208153e-05  0.01911376\n",
       "JUN2001  0.003691380 0.03759923 -0.0107715268  0.000000e+00  0.02511260\n",
       "JUL2001  0.017871018 0.03964184  0.0041325025 -1.208167e-05 -0.01092715\n",
       "        exhkus        exinus       exjpus       exkous       exmaus ...\n",
       "FEB2001  1.282084e-05 -0.001072731 -0.003771321 -0.015542616 0      ...\n",
       "MAR2001  0.000000e+00  0.001932990  0.045427170  0.030777827 0      ...\n",
       "APR2001 -7.692406e-05  0.003001072  0.018599292  0.028147529 0      ...\n",
       "MAY2001  7.692998e-05  0.003419534 -0.016159005 -0.021735856 0      ...\n",
       "JUN2001 -2.564135e-05  0.001916933  0.004763078 -0.002964047 0      ...\n",
       "JUL2001  2.564201e-05  0.002976190  0.017572538  0.007868422 0      ...\n",
       "        exsius      exsfus       exslus        exsdus       exszus     \n",
       "FEB2001 0.003164557  0.005502275  0.0151806415  0.027478664  0.02336707\n",
       "MAR2001 0.017034700  0.009793643 -0.0161356959  0.030743042  0.01330457\n",
       "APR2001 0.021768554  0.022828564  0.0288697072  0.015112022  0.01318902\n",
       "MAY2001 0.001269456 -0.012304569  0.0299642877  0.014485226  0.02317436\n",
       "JUN2001 0.001598589  0.010101643 -0.0052505284  0.042670969  0.01871292\n",
       "JUL2001 0.003467254  0.018599169 -0.0006307333 -0.003029741 -0.01601703\n",
       "        extaus       exthus        exukus       exvzus     exeuus      \n",
       "FEB2001 -0.010497965 -0.0112169459  0.017287234 0.00000000  0.018563660\n",
       "MAR2001  0.009031859  0.0310090238  0.005519245 0.01428571  0.013438881\n",
       "APR2001  0.009778677  0.0342366100  0.006788964 0.00000000  0.017620345\n",
       "MAY2001  0.007953614  0.0006814085  0.005738881 0.00000000  0.019725098\n",
       "JUN2001  0.033882480 -0.0057550796  0.017546362 0.01408451  0.026083151\n",
       "JUL2001  0.014361454  0.0083511919 -0.009112575 0.00000000 -0.009809776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99f5b49",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>119</li>\n",
       "\t<li>23</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 119\n",
       "\\item 23\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 119\n",
       "2. 23\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 119  23"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(fx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d6019",
   "metadata": {},
   "source": [
    "#### 1. Make a time series plot for all foreign exchanges. Do you see any patterns? For example, do some currencies show similar movements? Try kmeans clustering and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ae6ebf",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(tidyverse): there is no package called ‘tidyverse’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(tidyverse): there is no package called ‘tidyverse’\nTraceback:\n",
      "1. library(tidyverse)"
     ]
    }
   ],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "# plot the data\n",
    "\n",
    "library(tidyverse)\n",
    "library(lubridate)\n",
    "\n",
    "# Convert row names to a date format\n",
    "dates <- rownames(fx) %>%\n",
    "  str_replace(\"([A-Z]+)(\\\\d{4})\", \"\\\\2-\\\\1-01\") %>%\n",
    "  ymd()\n",
    "\n",
    "# Convert data to long format for plotting\n",
    "fx_data_long <- fx %>%\n",
    "  as.data.frame() %>%\n",
    "  mutate(date = dates) %>%\n",
    "  pivot_longer(cols = -date, names_to = \"currency\", values_to = \"exchange_rate\")\n",
    "\n",
    "# Plot time series\n",
    "fx_data_long %>%\n",
    "  ggplot(aes(x = date, y = exchange_rate, color = currency)) +\n",
    "  geom_line() +\n",
    "  labs(title = \"Monthly Foreign Exchange Rates\", x = \"Date\", y = \"Exchange Rate\") +\n",
    "  theme_minimal()\n",
    "\n",
    "# clustering\n",
    "\n",
    "# Normalize the data\n",
    "fx_data_normalized <- scale(fx)\n",
    "\n",
    "# Perform k-means clustering\n",
    "set.seed(123) # for reproducibility\n",
    "clusters <- kmeans(fx_data_normalized, centers = 5) # You can adjust the number of centers\n",
    "\n",
    "# Add cluster results back to the data\n",
    "fx_data_clustered <- cbind(fx, cluster = clusters$cluster)\n",
    "\n",
    "# Plot clusters\n",
    "fx_data_clustered %>%\n",
    "  as.data.frame() %>%\n",
    "  mutate(date = dates) %>%\n",
    "  pivot_longer(cols = -c(date, cluster), names_to = \"currency\", values_to = \"exchange_rate\") %>%\n",
    "  ggplot(aes(x = date, y = exchange_rate, color = as.factor(cluster))) +\n",
    "  geom_line() +\n",
    "  labs(title = \"Clustering of Monthly Foreign Exchange Rates\", x = \"Date\", y = \"Exchange Rate\", color = \"Cluster\") +\n",
    "  theme_minimal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "811d62e7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR COMMENT HERE\n",
    "\n",
    "# These clusters reveal how different countries currencies respond to global economic conditions, \n",
    "# regional influences, and policy decisions. For instance, currencies in clusters with less volatility might be \n",
    "# from economies with strong central bank policies or from more developed economies. Conversely, those in more \n",
    "# volatile clusters could be from emerging markets or countries experiencing economic difficulties. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f6191",
   "metadata": {},
   "source": [
    "#### 2. Discuss correlation amongst dimensions of fx. How does this relate to the applicability of factor modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a99e58",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "\n",
    "library(corrplot)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "cor_matrix <- cor(fx)\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "corrplot(cor_matrix, method = \"circle\", type = \"upper\", order = \"hclust\",\n",
    "         tl.col = \"black\", tl.srt = 45, addCoef.col = \"black\", \n",
    "         title = \"Correlation Matrix of FX Rates\")\n",
    "\n",
    "# Perform PCA\n",
    "fx_pca <- prcomp(fx, scale. = TRUE)\n",
    "summary(fx_pca)\n",
    "\n",
    "# Scree plot to visualize the importance of components\n",
    "screeplot(fx_pca, type = \"lines\", main = \"Scree Plot of PCA\")\n",
    "\n",
    "# Plot the loadings of the first two principal components\n",
    "biplot(fx_pca, cex = 0.7, col = c(\"red\", \"blue\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dfdc644",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>exeuus</th><th scope=col>exhkus</th><th scope=col>excaus</th><th scope=col>exmxus</th><th scope=col>exukus</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>exeuus</th><td>1.0000000 </td><td> 0.1833495</td><td>0.5048342 </td><td> 0.2617354</td><td>0.7335453 </td></tr>\n",
       "\t<tr><th scope=row>exhkus</th><td>0.1833495 </td><td> 1.0000000</td><td>0.1682997 </td><td>-0.1734493</td><td>0.1145097 </td></tr>\n",
       "\t<tr><th scope=row>excaus</th><td>0.5048342 </td><td> 0.1682997</td><td>1.0000000 </td><td> 0.5238391</td><td>0.4927518 </td></tr>\n",
       "\t<tr><th scope=row>exmxus</th><td>0.2617354 </td><td>-0.1734493</td><td>0.5238391 </td><td> 1.0000000</td><td>0.3203351 </td></tr>\n",
       "\t<tr><th scope=row>exukus</th><td>0.7335453 </td><td> 0.1145097</td><td>0.4927518 </td><td> 0.3203351</td><td>1.0000000 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & exeuus & exhkus & excaus & exmxus & exukus\\\\\n",
       "\\hline\n",
       "\texeuus & 1.0000000  &  0.1833495 & 0.5048342  &  0.2617354 & 0.7335453 \\\\\n",
       "\texhkus & 0.1833495  &  1.0000000 & 0.1682997  & -0.1734493 & 0.1145097 \\\\\n",
       "\texcaus & 0.5048342  &  0.1682997 & 1.0000000  &  0.5238391 & 0.4927518 \\\\\n",
       "\texmxus & 0.2617354  & -0.1734493 & 0.5238391  &  1.0000000 & 0.3203351 \\\\\n",
       "\texukus & 0.7335453  &  0.1145097 & 0.4927518  &  0.3203351 & 1.0000000 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | exeuus | exhkus | excaus | exmxus | exukus |\n",
       "|---|---|---|---|---|---|\n",
       "| exeuus | 1.0000000  |  0.1833495 | 0.5048342  |  0.2617354 | 0.7335453  |\n",
       "| exhkus | 0.1833495  |  1.0000000 | 0.1682997  | -0.1734493 | 0.1145097  |\n",
       "| excaus | 0.5048342  |  0.1682997 | 1.0000000  |  0.5238391 | 0.4927518  |\n",
       "| exmxus | 0.2617354  | -0.1734493 | 0.5238391  |  1.0000000 | 0.3203351  |\n",
       "| exukus | 0.7335453  |  0.1145097 | 0.4927518  |  0.3203351 | 1.0000000  |\n",
       "\n"
      ],
      "text/plain": [
       "       exeuus    exhkus     excaus    exmxus     exukus   \n",
       "exeuus 1.0000000  0.1833495 0.5048342  0.2617354 0.7335453\n",
       "exhkus 0.1833495  1.0000000 0.1682997 -0.1734493 0.1145097\n",
       "excaus 0.5048342  0.1682997 1.0000000  0.5238391 0.4927518\n",
       "exmxus 0.2617354 -0.1734493 0.5238391  1.0000000 0.3203351\n",
       "exukus 0.7335453  0.1145097 0.4927518  0.3203351 1.0000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here is what I get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a14a2c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# By examining the correlation matrix, we can identify which currencies move together against the US dollar. \n",
    "# High positive correlations suggest that currencies tend to strengthen or weaken together relative to the US \n",
    "# dollar, possibly due to similar economic conditions or geographic proximity.\n",
    "\n",
    "# This analysis will give a clear idea about the interdependencies among the foreign exchange rates and the \n",
    "# feasibility of reducing the dimensionality of the dataset through factor models, which could be crucial for \n",
    "# forecasting and risk management in financial applications and economic difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842b8d4",
   "metadata": {},
   "source": [
    "#### 3. Fit, plot, and interpret the latent factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca19fee",
   "metadata": {},
   "source": [
    "We saw in class that the solution to factor models is given by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8536f7d8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(repr)\n",
    "\n",
    "# Change plot size to 4 x 3\n",
    "options(repr.plot.width=4, repr.plot.height=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42735a31",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "# Fit a PCA on the big matrix Y_it, and do the variance plot\n",
    "\n",
    "fx_pca <- prcomp(fx, scale. = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f4a85",
   "metadata": {},
   "source": [
    "Following lecture codes, we produce the following plot of PCs and color them by the row indices (i.e., year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3563c537",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title \"% change in rates\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fxdir <- predict(fxpca)\n",
    "plot(fxdir[,1:2], pch=21, bg=terrain.colors(120)[120:1], main=\"% change in rates\")\n",
    "legend(\"topleft\", fill=terrain.colors(3), legend=c(\"2010\",\"2005\",\"2001\"), bty=\"n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa01fe8",
   "metadata": {},
   "source": [
    "#### 3.1. Hmm...This does not look like a great way to visualize time series data. Plot the first three PCs over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "042c3fd2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "\n",
    "# Extract the scores of the first three principal components\n",
    "pc_scores <- fx_pca$x[, 1:3]\n",
    "\n",
    "# Create a dataframe with dates and principal components\n",
    "dates <- rownames(fx) %>%\n",
    "  str_replace(\"([A-Z]+)(\\\\d{4})\", \"\\\\2-\\\\1-01\") %>%\n",
    "  ymd()\n",
    "\n",
    "pc_df <- data.frame(date = dates, PC1 = pc_scores[, 1], PC2 = pc_scores[, 2], PC3 = pc_scores[, 3])\n",
    "\n",
    "# Convert to long format for plotting\n",
    "pc_df_long <- pivot_longer(pc_df, cols = -date, names_to = \"Principal Component\", values_to = \"Score\")\n",
    "\n",
    "# Plot the principal components\n",
    "ggplot(pc_df_long, aes(x = date, y = Score, color = `Principal Component`)) +\n",
    "  geom_line() +\n",
    "  labs(title = \"Principal Component Analysis of FX Rates\",\n",
    "       x = \"Date\",\n",
    "       y = \"Component Scores\") +\n",
    "  theme_minimal()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ad36d",
   "metadata": {},
   "source": [
    "Before further analysis with the data, let's look for outliers in the data. Read returns for regression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ab2bb50",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "SP <- as.matrix(read.csv(\"./fx/sp500.csv\",row.names=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0397b96a",
   "metadata": {},
   "source": [
    "Consider the first three PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a95607",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary(sp500pcr <- lm(SP ~ ., data=data.frame(fxdir[,1:3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed27d6c8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(SP, sp500pcr$fitted, pch=21, bg=terrain.colors(120)[120:1],\n",
    "xlab=\"SP500 monthly returns\", ylab=\"fitted values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b602a2",
   "metadata": {},
   "source": [
    "There is a big outlier on lehman tanking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7802bd26",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "out <- which.min(sp500pcr$fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405e964",
   "metadata": {},
   "source": [
    "Rerun everything without the outlier (fine if you did not do this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58abbc1e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "SP <- SP[-out]\n",
    "fx <- fx[-out,]\n",
    "\n",
    "fxpca <- prcomp(fx, scale=TRUE)\n",
    "fxdir <- as.data.frame(predict(fxpca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c2685fb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "# examine the factor loadings\n",
    "\n",
    "summary(sp500pcr <- lm(SP ~ ., data=data.frame(fxdir[,1:3])))\n",
    "\n",
    "plot(SP, sp500pcr$fitted, pch=21, bg=terrain.colors(120)[120:1],\n",
    "     xlab=\"SP500 monthly returns\", ylab=\"fitted values\")\n",
    "\n",
    "# Identify the big outlier (e.g., Lehman collapse)\n",
    "out <- which.min(sp500pcr$fitted)  # Looking for the most negative fit\n",
    "\n",
    "# Remove the outlier and rerun everything\n",
    "SP_clean <- SP[-out]\n",
    "fx_clean <- fx[-out,]  # Assuming 'fx_data' contains your original FX data\n",
    "\n",
    "# Redo PCA on the cleaned FX data\n",
    "fxpca_clean <- prcomp(fx_clean, scale = TRUE)\n",
    "fxdir_clean <- as.data.frame(predict(fxpca_clean))\n",
    "\n",
    "# Fit the linear model again without the outlier\n",
    "sp500pcr_clean <- lm(SP_clean ~ ., data = fxdir_clean[, 1:3])\n",
    "\n",
    "# Now, examine the factor loadings of the cleaned data\n",
    "summary(sp500pcr_clean)\n",
    "\n",
    "# Visualize the new fitted values\n",
    "plot(SP_clean, sp500pcr_clean$fitted, pch = 21, bg = terrain.colors(120)[120:1],\n",
    "     xlab = \"S&P 500 Monthly Returns (Clean)\", ylab = \"Fitted Values from PCA (Clean)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f43925f5",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR COMMENT HERE\n",
    "# interpret factors \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e51e50",
   "metadata": {},
   "source": [
    "#### 4. Regress SP500 returns onto currency movement factors, using both \"glm on first K\" and lasso techniques. Use the results to add to your factor interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd198f",
   "metadata": {},
   "source": [
    "First, let's run principal component regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9147830",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'gamlr' was built under R version 3.6.3\"Loading required package: Matrix\n"
     ]
    }
   ],
   "source": [
    "library(gamlr)\n",
    "\n",
    "## Get glm fits on 1:20 factors\n",
    "kfits <- lapply(1:20, function(K) glm(SP ~ ., data=fxdir[,1:K,drop=FALSE]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a26d895",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "3"
      ],
      "text/latex": [
       "3"
      ],
      "text/markdown": [
       "3"
      ],
      "text/plain": [
       "[1] 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "3"
      ],
      "text/latex": [
       "3"
      ],
      "text/markdown": [
       "3"
      ],
      "text/plain": [
       "[1] 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aicc <- sapply(kfits, AICc)\n",
    "which.min(aicc)\n",
    "\n",
    "bic <- sapply(kfits, BIC)\n",
    "which.min(bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c29ecf",
   "metadata": {},
   "source": [
    "Looks like 3 is best, so use that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f72b297",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "# run a regression using 3 factors, output the summary \n",
    "\n",
    "library(glmnet)  \n",
    "library(MASS)   \n",
    "\n",
    "# Generalized Linear Models for 1 to 20 factors\n",
    "kfits <- lapply(1:20, function(K) {\n",
    "  glm(SP ~ ., data = fxdir[, 1:K, drop = FALSE])\n",
    "})\n",
    "\n",
    "# Calculate and compare AICc and BIC\n",
    "aicc <- sapply(kfits, AICc)\n",
    "bic <- sapply(kfits, BIC)\n",
    "\n",
    "# Determine the best model according to AICc and BIC\n",
    "best_aicc <- which.min(aicc)\n",
    "best_bic <- which.min(bic)\n",
    "\n",
    "cat(\"Best model by AICc is with\", best_aicc, \"factors\\n\")\n",
    "cat(\"Best model by BIC is with\", best_bic, \"factors\\n\")\n",
    "\n",
    "best_k <- best_aicc\n",
    "\n",
    "# Fit and summary of the selected model\n",
    "selected_glm <- glm(SP ~ ., data = fxdir[, 1:best_k, drop = FALSE])\n",
    "summary(selected_glm)\n",
    "\n",
    "# Lasso Regression for comparison\n",
    "x_matrix <- as.matrix(fxdir[, 1:best_k])\n",
    "y_vector <- SP\n",
    "\n",
    "lasso_fit <- glmnet(x_matrix, y_vector, alpha = 1)\n",
    "cv_lasso <- cv.glmnet(x_matrix, y_vector, type.measure = \"mse\", alpha = 1)\n",
    "\n",
    "# Plot the Lasso CV curve\n",
    "plot(cv_lasso)\n",
    "\n",
    "# Extract the best lambda (regularization parameter)\n",
    "best_lambda <- cv_lasso$lambda.min\n",
    "cat(\"Best lambda for Lasso is\", best_lambda, \"\\n\")\n",
    "\n",
    "# Lasso model at best lambda\n",
    "lasso_best <- glmnet(x_matrix, y_vector, alpha = 1, lambda = best_lambda)\n",
    "coef(lasso_best)\n",
    "\n",
    "# Summary of Lasso model\n",
    "lasso_summary <- predict(lasso_best, type = \"coefficients\")\n",
    "print(lasso_summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eac5c5a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR COMMENT HERE\n",
    "# refine your interpretation of factors\n",
    "\n",
    "# The analysis using both Generalized Linear Models (GLM) and Lasso regression reveals significant influences \n",
    "# of the first three principal components derived from currency exchange rates on the S&P 500 returns. \n",
    "# PC1 and PC3 positively impact the S&P 500, suggesting they capture beneficial economic indicators or favorable \n",
    "# exchange rate conditions that support upward market trends. In contrast, PC2 has a negative effect, indicating \n",
    "# it represents risk factors or adverse economic conditions that deter investment in the stock market. The \n",
    "# consistency of these results across both GLM and Lasso models underscores the robustness of these findings and \n",
    "# highlights the relevance of these components in understanding the interplay between global currency movements \n",
    "# and domestic stock market performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf0bd3",
   "metadata": {},
   "source": [
    "Alternatively, try LASSO to do automatic factor selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1272242f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lassoPCR <- cv.gamlr(x=fxdir, y=SP)\n",
    "plot(lassoPCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86cd4b40",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24 x 1 sparse Matrix of class \"dgCMatrix\"\n",
       "                  seg13\n",
       "intercept  0.0018826525\n",
       "PC1        0.0010959473\n",
       "PC2       -0.0047590998\n",
       "PC3        0.0008823832\n",
       "PC4        .           \n",
       "PC5        .           \n",
       "PC6        .           \n",
       "PC7        .           \n",
       "PC8        .           \n",
       "PC9        .           \n",
       "PC10       .           \n",
       "PC11       .           \n",
       "PC12       .           \n",
       "PC13       .           \n",
       "PC14       .           \n",
       "PC15       .           \n",
       "PC16       .           \n",
       "PC17       .           \n",
       "PC18       .           \n",
       "PC19       .           \n",
       "PC20       0.0024219521\n",
       "PC21       .           \n",
       "PC22       .           \n",
       "PC23       .           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "coef(lassoPCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7b666",
   "metadata": {},
   "source": [
    "It agrees on the need for those first three. but adds in some others. The fact that we’ve loaded on PC20 is a bit strange. This doesn’t normally happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481da0fa",
   "metadata": {},
   "source": [
    "#### 5. Another way to deal with multicolinearity with many regressors is variable selection. Fit LASSO to the original covariates and describe how it differs from Principal component regression (PCR) here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef4f3d0f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "\n",
    "# Convert to matrix format as glmnet requires matrix input\n",
    "x_matrix <- as.matrix(fx)\n",
    "y_vector <- SP  # Ensure SP is loaded and aligned with fx_data\n",
    "\n",
    "# Fit LASSO model\n",
    "lasso_fit <- glmnet(x_matrix, y_vector, alpha = 1)  # alpha=1 for LASSO\n",
    "cv_lasso <- cv.glmnet(x_matrix, y_vector, type.measure = \"mse\", alpha = 1)\n",
    "\n",
    "# Plot the coefficient path\n",
    "plot(lasso_fit, xvar = \"lambda\", label = TRUE)\n",
    "\n",
    "# Identify the lambda that minimizes the cross-validated MSE\n",
    "best_lambda <- cv_lasso$lambda.min\n",
    "cat(\"Best lambda for LASSO is\", best_lambda, \"\\n\")\n",
    "\n",
    "# Examine coefficients at the best lambda\n",
    "best_coef <- coef(lasso_fit, s = best_lambda)\n",
    "print(best_coef)\n",
    "\n",
    "# Visualize the cross-validation plot to show MSE for different lambdas\n",
    "plot(cv_lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd72ba96",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR COMMENT HERE\n",
    "\n",
    "# LASSO and PCR are distinct approaches to handling multicollinearity and high-dimensional data in regression models. \n",
    "# PCR reduces dimensionality by transforming the original variables into a set of principal components, \n",
    "# which are then used as predictors, effectively condensing the information but often at the cost of \n",
    "# interpretability since the components are combinations of all original variables. LASSO, in contrast, \n",
    "# incorporates a regularization term that penalizes the absolute size of the coefficients, leading to a model \n",
    "# where less influential predictors are completely excluded by shrinking their coefficients to zero, thus \n",
    "# enhancing model interpretability and focusing on the most significant variables. While PCR is geared to \n",
    "# dimensionality reduction, LASSO emphasizes variable selection, making it suitable for scenarios where \n",
    "# understanding and selecting the key predictors are critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba1444",
   "metadata": {},
   "source": [
    "#### 6. Now, suppose a manager asks you to make a forecast for Hong Kong dollar (HKD) returns in the next two years. To this end, you first consider using past time series to predict future values. \n",
    "\n",
    "You write down the following model:\n",
    "$$Y_{1,t} = \\alpha + \\beta Y_{i,t-1} + U_{1, t}$$\n",
    "where $Y_{1,t}$ denotes the return to HKD in year $t$, and $U_{1,t}$ is normal $(0,\\sigma^2)$ independent of $Y_{1,t-1}, Y_{1, t-2}, \\cdots$. \n",
    "\n",
    "Explain how to consistently estimate $\\alpha$ and $\\beta$ in this model. Write down the expression of the best predictor $\\hat{Y}_{1, T+1}$ in this model, as a function of the estimated $\\alpha$ and $\\beta$, and of $Y_{1,T}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d1f93e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# Simulate an analysis with data \"hk return\"\n",
    "\n",
    "library(forecast)\n",
    "\n",
    "# Simulate a hk return data\n",
    "hk_returns <- read.csv(\"hk_returns.csv\")$HKD_Returns\n",
    "\n",
    "# Prepare the data: Create a lagged version of HKD returns\n",
    "hk_returns_lagged <- lag(hk_returns, -1)\n",
    "\n",
    "# Remove NA values which are introduced by lagging\n",
    "data <- na.omit(data.frame(current = hk_returns[-length(hk_returns)], lagged = hk_returns_lagged[-1]))\n",
    "\n",
    "# Fit the linear model\n",
    "model <- lm(current ~ lagged, data = data)\n",
    "\n",
    "# Display the model summary for parameter estimates\n",
    "summary(model)\n",
    "\n",
    "# Predict the next year return using the estimated model parameters\n",
    "next_year_prediction <- coef(model)[\"(Intercept)\"] + coef(model)[\"lagged\"] * tail(data$lagged, 1)\n",
    "\n",
    "# Output the prediction\n",
    "print(next_year_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741a4d8",
   "metadata": {},
   "source": [
    "#### 7. Another way to forecast HKD returns is taking advantage of all other currency returns. \n",
    "Consider the 1-factor model:\n",
    "$$Y_{i,t} = \\mu_i f_t + U_{i,t}$$\n",
    "In addition, assume AR(1) process for $f_t$:\n",
    "$$f_t = c + \\gamma f_{t-1} + v_t$$\n",
    "where $v_t$ is normal $(0,s^2)$ independent of $f_{t-1}, f_{t-2}, \\cdots$\n",
    "\n",
    "Explain how to consistently estimate $c$ and $\\gamma$ in this model. Write the expression of the best predictor $\\hat{Y}_{1, T+1}$ in this setting, as a function of the estimated $c, \\gamma, \\mu_1$, and $f_T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25133735",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# Estimate the parameter\n",
    "# Assuming 'returns' is matrix of currency returns dataset\n",
    "library(stats)\n",
    "returns <- as.matrix(read.csv(\"files.csv\"))\n",
    "\n",
    "# Estimating the factor using PCA\n",
    "pca_results <- prcomp(returns, scale. = TRUE)\n",
    "f_t <- pca_results$x[, 1]  # Assuming the first principal component is f_t\n",
    "\n",
    "# Fit AR(1) model to the factor scores\n",
    "ar1_model <- arima(f_t, order = c(1, 0, 0))\n",
    "c <- ar1_model$coef[1]\n",
    "gamma <- ar1_model$coef[2]\n",
    "\n",
    "# Display estimated coefficients\n",
    "print(c)\n",
    "print(gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdb4df1",
   "metadata": {},
   "source": [
    "### Q2. Congressional speech\n",
    "We introduced roll call voting in US congress in the week of lecture on PCA. In this exercise, instead of voting records, we take a look at congressional speech to understand what politicians talked about during meeting sessions and what these reveal about their political ideology.\n",
    "\n",
    "`textir` contains `congress109` data: counts for 1k phrases used by each of 529 members of the 109th US congress. The phrase counts are in `congress109Counts`, a big sparse matrix. We also have `congress109Ideology`, a data.frame containing some information about each speaker. This includes some partisan metrics:\n",
    "* party: categorical, taking values in {Republican, Democrat, or Independent}\n",
    "\n",
    "* repshare: numeric, share of constituents voting for Bush in 2004\n",
    "\n",
    "* Common Scores [cs1, cs2]: basically, the two principal components of roll-call votes (see lecture 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc9b70",
   "metadata": {},
   "source": [
    "#### 1. Fit k-means to speech text for $K$ in 5, 10, 15, 20, 25. Use BIC to choose the $K$ and interpret the selected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "32b43dc8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "require(textir, quietly = TRUE) # to get the data\n",
    "require(maptpx, quietly = TRUE) # for the topics function\n",
    "data(congress109)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c08bae",
   "metadata": {},
   "source": [
    "standardize the data before fitting k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7d7a17e1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "fs <- scale(as.matrix( congress109Counts/rowSums(congress109Counts) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a8b2a9f7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>529</li>\n",
       "\t<li>1000</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 529\n",
       "\\item 1000\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 529\n",
       "2. 1000\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  529 1000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a66c2c6e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "# fit kmeans for a grid of K\n",
    "library(textir)\n",
    "library(maptpx)  # This might not be necessary unless using specific functions from this package\n",
    "library(stats)   # For kmeans and scale\n",
    "\n",
    "# Load the data\n",
    "data(congress109)\n",
    "\n",
    "# Preprocess the data\n",
    "phrase_counts <- as.matrix(congress109Counts)\n",
    "scaled_counts <- scale(phrase_counts / rowSums(phrase_counts))\n",
    "\n",
    "# Define a range of K values to test\n",
    "K_values <- c(5, 10, 15, 20, 25)\n",
    "results <- list()\n",
    "\n",
    "# Fit k-means clustering for each K and calculate BIC\n",
    "for (K in K_values) {\n",
    "  set.seed(123)  # Ensure reproducibility\n",
    "  km <- kmeans(scaled_counts, centers=K, nstart=25)\n",
    "  \n",
    "  # Calculate total within-cluster sum of squares\n",
    "  wss <- sum(km$withinss)\n",
    "  \n",
    "  # Estimate BIC, assuming independence (might require adjustment based on actual data properties)\n",
    "  n <- nrow(scaled_counts)\n",
    "  p <- ncol(scaled_counts)\n",
    "  log_likelihood <- -0.5 * n * log(wss)\n",
    "  BIC <- log(n) * K * p - 2 * log_likelihood\n",
    "  \n",
    "  # Store results\n",
    "  results[[as.character(K)]] <- list(km = km, BIC = BIC)\n",
    "}\n",
    "\n",
    "\n",
    "# use IC to choose best K\n",
    "# Find the best K by minimum BIC\n",
    "BIC_values <- sapply(results, function(x) x$BIC)\n",
    "best_K <- names(which.min(BIC_values))\n",
    "best_model <- results[[best_K]]\n",
    "\n",
    "# Print best K and its BIC\n",
    "cat(\"Best K by BIC:\", best_K, \"with BIC:\", min(BIC_values), \"\\n\")\n",
    "print(best_model$km$centers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb4e94",
   "metadata": {},
   "source": [
    "There’s no clear way to choose. . . we’ll go with 20. you could have gone with whatever makes sense to you:\n",
    "that’s how I recommend working here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6932e561",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "# interpret your chosen model\n",
    "\n",
    "# Selecting K = 20 for the k-means clustering on the scaled congressional speech data provides a detailed and \n",
    "# nuanced segmentation of the speeches into 20 distinct clusters. This choice allows for a broader exploration of \n",
    "# the variety in phrase usage among the members, potentially reflecting diverse political ideologies, regional \n",
    "# differences, topical interests, or communication styles. By examining the centroids of this model, we can \n",
    "# identify dominant themes or specific issues that are more prevalent within certain clusters. For example, \n",
    "# one cluster might focus on economic policy, while another highlights social issues, indicating the priorities \n",
    "# or typical rhetoric used by different groups of legislators. This segmentation helps in understanding the \n",
    "# multifaceted nature of political discourse in the 109th U.S. Congress, offering insights into how different \n",
    "# topics are emphasized across party lines and individual members.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de9aa8",
   "metadata": {},
   "source": [
    "#### 2. Fit a topic model for the speech counts. Use Bayes factors to choose the number of topics, and interpret your chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8957084",
   "metadata": {},
   "source": [
    "Topic modelling: we’ll choose the number of topics. Recall: BF is like exp(-BIC), so you choose the bigggest\n",
    "BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4c3ace9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "\n",
    "# fit topic models\n",
    "# check out a few topics characterized by word distribution\n",
    "\n",
    "\n",
    "library(textir)\n",
    "library(topicmodels)\n",
    "\n",
    "# Preprocess the data\n",
    "phrase_counts <- as.matrix(congress109Counts)\n",
    "scaled_counts <- scale(phrase_counts / rowSums(phrase_counts))\n",
    "\n",
    "# Define a range of topic numbers to test\n",
    "topic_nums <- c(5, 10, 15, 20, 25)\n",
    "lda_models <- list()\n",
    "\n",
    "library(doParallel)\n",
    "registerDoParallel(cores = detectCores() - 8)\n",
    "\n",
    "# Fit LDA models more efficiently\n",
    "for (K in topic_nums) {\n",
    "  lda_models[[as.character(K)]] <- LDA(phrase_counts, k=K, method=\"Gibbs\", control=list(seed=1234, burnin=500, thin=100, iter=1000))\n",
    "}\n",
    "\n",
    "# Define a function to compute the Bayes Factor between two models\n",
    "compute_bayes_factor <- function(lda1, lda2) {\n",
    "  log_likelihood1 <- sum(logLik(lda1))\n",
    "  log_likelihood2 <- sum(logLik(lda2))\n",
    "  BF <- exp(log_likelihood1 - log_likelihood2)\n",
    "  return(BF)\n",
    "}\n",
    "\n",
    "# Compare models and select the one with the highest Bayes Factor\n",
    "bayes_factors <- sapply(2:length(topic_nums), function(i) {\n",
    "  compute_bayes_factor(lda_models[[as.character(topic_nums[i])]], lda_models[[as.character(topic_nums[i-1])]])\n",
    "})\n",
    "\n",
    "best_K_index <- which.max(bayes_factors) + 1  # Account for 1-based indexing\n",
    "best_K <- topic_nums[best_K_index]\n",
    "best_model <- lda_models[[as.character(best_K)]]\n",
    "\n",
    "cat(\"Chosen model has\", best_K, \"topics\\n\")\n",
    "\n",
    "# Explore the chosen model by examining the topic-word distributions\n",
    "terms(best_model, 10)  # Shows the top 10 terms for each topic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee935550",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "# interpret your chosen model\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA) is from the topicmodels package to generate topic models for different values  \n",
    "# of K (number of topics)\n",
    "\n",
    "# The Bayes Factor is computed to compare the relative likelihood of consecutive models and identify which \n",
    "# number of topics provides a better fit.\n",
    "\n",
    "# The terms function provides a simple way to view the top 10 words associated with each topic, \n",
    "# giving an intuitive understanding of the themes captured by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d33937",
   "metadata": {},
   "source": [
    "You can tell any story; I see big gop and dem topics 1 and 2, then issue specific stuff. If you ran a different\n",
    "configuration (e.g., $K = 5 * (1 : 5)$), then you might have ended up selecting or working with a totally different\n",
    "set of topics. Topic models are what we call ‘unidentified’ – in practice, this means that the fitting algorithms\n",
    "give different answers depending upon where you start them. The topics algorithm fits topics sequentially, so\n",
    "that your fit at K = 10 is used to derive a good starting location for your fit at, say, K=15. It is not a hard\n",
    "rule, but I think you tend to get better topics starting from smaller K and taking smaller steps between K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a2c498",
   "metadata": {},
   "source": [
    "Other fun stuff to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6041c360",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>3</dt>\n",
       "\t\t<dd>0.221970582988828</dd>\n",
       "\t<dt>1</dt>\n",
       "\t\t<dd>0.271937113349208</dd>\n",
       "\t<dt>6</dt>\n",
       "\t\t<dd>0.295565928440861</dd>\n",
       "\t<dt>11</dt>\n",
       "\t\t<dd>0.422653982058838</dd>\n",
       "\t<dt>7</dt>\n",
       "\t\t<dd>0.590291887237524</dd>\n",
       "\t<dt>9</dt>\n",
       "\t\t<dd>1.61916505662835</dd>\n",
       "\t<dt>8</dt>\n",
       "\t\t<dd>1.69682756306529</dd>\n",
       "\t<dt>2</dt>\n",
       "\t\t<dd>2.23464658008884</dd>\n",
       "\t<dt>5</dt>\n",
       "\t\t<dd>2.63271633223826</dd>\n",
       "\t<dt>10</dt>\n",
       "\t\t<dd>4.11749958925956</dd>\n",
       "\t<dt>4</dt>\n",
       "\t\t<dd>8.23135270989357</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[3] 0.221970582988828\n",
       "\\item[1] 0.271937113349208\n",
       "\\item[6] 0.295565928440861\n",
       "\\item[11] 0.422653982058838\n",
       "\\item[7] 0.590291887237524\n",
       "\\item[9] 1.61916505662835\n",
       "\\item[8] 1.69682756306529\n",
       "\\item[2] 2.23464658008884\n",
       "\\item[5] 2.63271633223826\n",
       "\\item[10] 4.11749958925956\n",
       "\\item[4] 8.23135270989357\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "3\n",
       ":   0.2219705829888281\n",
       ":   0.2719371133492086\n",
       ":   0.29556592844086111\n",
       ":   0.4226539820588387\n",
       ":   0.5902918872375249\n",
       ":   1.619165056628358\n",
       ":   1.696827563065292\n",
       ":   2.234646580088845\n",
       ":   2.6327163322382610\n",
       ":   4.117499589259564\n",
       ":   8.23135270989357\n",
       "\n"
      ],
      "text/plain": [
       "        3         1         6        11         7         9         8         2 \n",
       "0.2219706 0.2719371 0.2955659 0.4226540 0.5902919 1.6191651 1.6968276 2.2346466 \n",
       "        5        10         4 \n",
       "2.6327163 4.1174996 8.2313527 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at party mean memberships\n",
    "DemO <- colMeans(tpcs$omega[congress109Ideology$party==\"D\",])\n",
    "RepO <- colMeans(tpcs$omega[congress109Ideology$party==\"R\",])\n",
    "sort(DemO/RepO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db734c77",
   "metadata": {},
   "source": [
    "1,3,6 are republican and 4,5,10 are strong dem. I can say this because, e.g., 1 has a low Dem/Rep ratio and 4\n",
    "has a high Dem/Rep ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43bcfb7",
   "metadata": {},
   "source": [
    "#### 3. Create a word cloud. Does it make sense to you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b967ef81",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'wordcloud' was built under R version 3.6.3\"Loading required package: RColorBrewer\n"
     ]
    }
   ],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "library(wordcloud)\n",
    "library(topicmodels)\n",
    "library(RColorBrewer)  \n",
    "\n",
    "# Get the terms with their weights (betas) from the model\n",
    "beta <- posterior(best_model)$terms\n",
    "\n",
    "# Sum the beta weights across topics to get overall importance of each term\n",
    "term_frequency <- colSums(beta)\n",
    "\n",
    "# Get term names\n",
    "term_names <- colnames(beta)\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud(words = term_names, freq = term_frequency, min.freq = 1,\n",
    "          max.words = 200, random.order = FALSE, rot.per = 0.35,\n",
    "          colors = brewer.pal(8, \"Dark2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "52aa7850",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# Creating a word cloud from the output of an LDA topic model can be quite useful for several reasons, \n",
    "# particularly when we want to visualize the most prominent words within the topics in a quick and visually \n",
    "# engaging way. Using a word cloud can certainly be a valuable part of our analysis, especially for initial \n",
    "# explorations and presentations. However, it should ideally be complemented with more detailed analyses and \n",
    "# visualizations that can offer deeper insights into the structure and content of your topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18565a",
   "metadata": {},
   "source": [
    "Now, let's connect the unsupersived clusters to partisanship. \n",
    "#### 4. Tabulate party membership by k-means cluster. Are there any non-partisan topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58883776",
   "metadata": {},
   "source": [
    "First, we can just table party by kmeans cluster (like red v. white wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "70e311de",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "\n",
    "# Add cluster labels to the party data\n",
    "congress109Ideology$cluster <- km$cluster\n",
    "\n",
    "# Create a contingency table of party by cluster\n",
    "party_cluster_table <- table(congress109Ideology$party, congress109Ideology$cluster)\n",
    "\n",
    "# Print the table\n",
    "print(party_cluster_table)\n",
    "\n",
    "# Optionally, you can use prop.table to see proportions instead of counts\n",
    "prop_party_cluster <- prop.table(party_cluster_table, margin = 1)  # Normalize by rows\n",
    "print(prop_party_cluster)\n",
    "\n",
    "library(ggplot2)\n",
    "\n",
    "# Melt the table for visualization\n",
    "library(reshape2)\n",
    "melted_data <- melt(party_cluster_table)\n",
    "\n",
    "# Create a bar plot\n",
    "ggplot(melted_data, aes(x = Var2, y = value, fill = Var1)) + \n",
    "  geom_bar(stat = \"identity\", position = position_dodge()) +\n",
    "  labs(x = \"Cluster\", y = \"Count\", fill = \"Party\") +\n",
    "  theme_minimal() +\n",
    "  ggtitle(\"Distribution of Party Membership by K-Means Cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5a68e95",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# There are any non-partisan topics in the k-means clusters.\n",
    "# Some clusters displays a relatively balanced mix of different party members like Republicans or Democrats, \n",
    "# it suggests that the topics or phrases prevalent in that cluster are not strongly associated with a \n",
    "# particular partisan viewpoint.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfab324",
   "metadata": {},
   "source": [
    "#### 5. Fit topic regressions for each of *party* and *repshare*. Compare to regression onto phrase relative frequencies and comment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14d27b4",
   "metadata": {},
   "source": [
    "First, fit a topic regression for `party'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "de3739f1",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 x 1 Matrix of class \"dgeMatrix\"\n",
      "             seg100\n",
      "intercept 0.8623905\n",
      "1         1.6729093\n",
      "2         0.9460308\n",
      "3         2.5362549\n",
      "4         0.4177292\n",
      "5         0.8548350\n",
      "6         2.1282841\n",
      "7         1.2315770\n",
      "8         0.9639157\n",
      "9         1.0000000\n",
      "10        0.6530959\n",
      "11        1.5327569\n"
     ]
    }
   ],
   "source": [
    "library(gamlr)\n",
    "\n",
    "# omega is the n x K matrix of document topic weights\n",
    "# i.e., how much of each doc is from each topic\n",
    "gop <- congress109Ideology[,\"party\"]==\"R\"\n",
    "partyreg <- gamlr(tpcs$omega, gop,\n",
    "family=\"binomial\") # don't forget: it's logistic regression!\n",
    "\n",
    "# odd multipliers for a 10% rise in topic in doc\n",
    "print(exp(coef(partyreg)*0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f85366d",
   "metadata": {},
   "source": [
    "#### 5.1 Interpret one of the coefficients in the output table above. Which topics has the strongest correlation with party affiliation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6aca9fd9",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# For topic 3, the coefficient's exponential is around 1.281. This implies that a 10% increase in the weight \n",
    "# of topic 3 in a document is associated with an increase in the odds of the speaker being a Republican by about \n",
    "# 28.1%. This significant change suggests that topic 3 likely contains language or themes that are strongly \n",
    "# correlated with Republican speakers, perhaps reflecting key issues or rhetoric that resonate more with \n",
    "# Republican values or policy focuses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425b0ea",
   "metadata": {},
   "source": [
    "Same thing, but for `repshare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272d0063",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in gamlr(tpcs$omega, congress109Ideology$repshare): could not find function \"gamlr\"\n",
     "output_type": "error",
     "traceback": [
      "Error in gamlr(tpcs$omega, congress109Ideology$repshare): could not find function \"gamlr\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "## INPUT YOUR CODE HERE\n",
    "\n",
    "# this is now linear regression\n",
    "repreg <- gamlr(tpcs$omega, congress109Ideology$repshare)\n",
    "\n",
    "# Print the coefficients\n",
    "print(coef(repshare_reg))\n",
    "\n",
    "# For interpretation: Print coefficients as changes for a 10% rise in topic in doc\n",
    "print(exp(coef(repshare_reg) * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2687d56b",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 x 1 sparse Matrix of class \"dgCMatrix\"\n",
      "                 seg83\n",
      "intercept  0.048615862\n",
      "1          0.013106779\n",
      "2         -0.014366331\n",
      "3          0.016909391\n",
      "4         -0.010495792\n",
      "5          .          \n",
      "6          0.017160002\n",
      "7          0.009301932\n",
      "8         -0.004165623\n",
      "9          .          \n",
      "10        -0.012949200\n",
      "11         0.008357165\n"
     ]
    }
   ],
   "source": [
    "# increase in repshare per 10% rise in topic in doc\n",
    "print(coef(repreg)*0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970ef96",
   "metadata": {},
   "source": [
    "The effects have the same direction (+/- sign) as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4f2f638c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Compare to the straight regression on phrases\n",
    "## INPUT YOUR CODE HERE\n",
    "\n",
    "library(stats)  # For lm function\n",
    "\n",
    "# Prepare the data\n",
    "phrase_freqs <- as.matrix(congress109Counts)\n",
    "repshare <- congress109Ideology$repshare\n",
    "\n",
    "# Fit a linear model\n",
    "phrase_reg <- lm(repshare ~ phrase_freqs)\n",
    "\n",
    "# Print the summary of the model to check coefficients and model performance\n",
    "summary(phrase_reg)\n",
    "\n",
    "library(gamlr)\n",
    "\n",
    "# Fit the regression model using topic weights\n",
    "topic_reg <- gamlr(tpcs$omega, repshare)\n",
    "\n",
    "# Print the summary of the model\n",
    "summary(topic_reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b689451",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR COMMENT HERE\n",
    "\n",
    "# In comparing the effectiveness of using k-means cluster assignments in congressional speeches for predicting \n",
    "# party affiliation and representative share repshare, the results reveal distinct outcomes. Logistic \n",
    "# regression on clusters effectively predicts party affiliation, demonstrating a clear link between specific \n",
    "# speech patterns and party lines with decent precision and high recall for Republicans. However, when using \n",
    "# these same clusters to predict repshare through linear regression, the model performs poorly, exhibiting \n",
    "# a significantly negative R², indicating a worse fit than a horizontal line. This suggests that while clusters \n",
    "# may capture partisan language effectively, they do not linearly correlate with the proportion of constituent \n",
    "# support, highlighting the limitations of using such clusters for predicting continuous outcomes like repshare. \n",
    "# Direct phrase frequency analysis might offer more nuanced insights, particularly for continuous variables, \n",
    "# by providing a direct measure of linguistic elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29d871",
   "metadata": {},
   "source": [
    "#### 5.2 Which phrases are cues for Republican vs Democrat speeches? In the example of hygiene grade prediction using online reviews, it is not clear ex ante that text features could be useful signals. Discuss why the text features (either raw phrases or topics) might be predictive of political ideology in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a8be7fb",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# INPUT YOUR ANSWER HERE\n",
    "\n",
    "# In the context of political speeches, text features such as raw phrases or topics derived from speeches can \n",
    "# be predictive of political ideology because language use often reflects underlying values, priorities, and \n",
    "# policy stances that are characteristic of political parties. For instance, Republicans and Democrats may \n",
    "# emphasize different issues like \"tax cuts\" versus \"social justice\" or use distinct framing techniques that \n",
    "# align with their ideological bases. These textual cues can serve as strong indicators of a speaker’s political \n",
    "# affiliation due to the consistent and systematic ways in which parties communicate their messages to resonate \n",
    "# with specific voter groups. Consequently, analyzing speech content can reveal patterns that distinguish \n",
    "# Republican from Democrat speeches, leveraging these differences as predictive markers in a similar way that \n",
    "# specific keywords in online reviews might indicate hygiene standards or service quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b1a88",
   "metadata": {},
   "source": [
    "#### 6. Measuring partisanship\n",
    "Topic model is one way of performing dimension reduction on high dimensional text data -- congressional speech in this exercise. Suppose someone asks to you come up with an index of partisanship (defined as segregation along party lines) based on how people speak in this congress session. What would you propose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98ae0146",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# To create an index of partisanship based on congressional speech, there is a methodology that first involves\n",
    "# using a topic modeling technique like LDA to reduce the high-dimensional text data into a set of topics. \n",
    "# Then, for each topic, I would calculate the frequency of topic occurrence or topic proportions among the \n",
    "# speeches of members from each political party. The index of partisanship for each topic could then be measured \n",
    "# by the divergence between these proportions, using metrics like the Kullback-Leibler divergence or Chi-squared \n",
    "# statistics. This would quantify how distinctively a topic is discussed among different parties. The overall \n",
    "# partisanship index could be an average or weighted sum of these divergences across all topics, providing a \n",
    "# single measure that reflects the degree of linguistic segregation along party lines in congressional speeches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a79b45",
   "metadata": {},
   "source": [
    "#### 7. You might have noticed from the previous analysis that Democrats and Republicans speak differently. Does this suggest a causal relationship between party and speech? If not, discuss one possible confounder and how you may control for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "96785530",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## INPUT YOUR ANSWER HERE\n",
    "\n",
    "# Observing differences in speech patterns between Democrats and Republicans does not necessarily imply a causal \n",
    "# relationship between party affiliation and speech style. One possible confounder in this relationship could be \n",
    "# the geographic region or state from which the congress members hail. Regions often have distinct political \n",
    "# cultures and issues, which could influence both the political leanings of their representatives and the topics \n",
    "# or language those representatives emphasize in their speeches. To control for this confounder, one could include \n",
    "# geographic or state-level variables as covariates in a regression model or perform a stratified analysis where \n",
    "# speech patterns are compared within the same geographic region across different parties. This approach helps \n",
    "# isolate the effect of party affiliation on speech from the regional influences, providing a clearer view of the \n",
    "# causal relationship.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3f373e",
   "metadata": {},
   "source": [
    "### Congrats, you are done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bce8f6",
   "metadata": {},
   "source": [
    "Before you go, here is one more thing to chew on. Topic model is a generative model of text and assumes latent factors (i.e., topics) that give rise to all the words. As social scientists, we are used to thinking in terms the micro-foundation of speeches. From the discrete choice perspective, at each utterance, the person is choosing from the set of vocabulary one particular word, in the same way as she/he is picking one yogurt among the many options on the shelves in the supermarket. Can we model speeches using a discrete choice model where the set of choices is huge? If so, how are the modeling assumptions different from LDA? Is one approach better than the other?\n",
    "\n",
    "Several economists do exactly that, although the motivation is not to compare with language models in the machine learning (or NLP) literature. They do demonstrate, however, that in the modeling of high-dimensional choices, penalization is crucial to estimating the model. The major observation the authors make is that using word frequencies in the naive way can  suffer from serious finite sample bias. When some words are only used by Democrats in the data, it does not necessarily suggest different *probabilities* of using that word across parties. Rather, it might well be due to sampling error. Another thing they find is that roll call voting and speech do not go hand in hand -- votes are polarized today as much as a hundred years ago, but speech only recently diverged.\n",
    "\n",
    "Read on in case you are interested!\n",
    "\n",
    "Reference: Gentzkow, M., Shapiro, J. M., & Taddy, M. (2019). Measuring group differences in high‐dimensional choices: method and application to congressional speech. *Econometrica*, 87(4), 1307-1340."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036be038",
   "metadata": {},
   "source": [
    "This is the end of class. I hope you enjoyed it and have learned a thing or two. Have a nice summer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb55c27",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### [HIDE THIS FROM STUDENTS] Further comments (see note-GentzkowTaddy.jpg in the same folder)\n",
    "\n",
    "- Why have this structural model when you can go reduced form? In the end, this is a prediction task right?\n",
    "\n",
    "A. Yes, we can take the other route and just do language model of speeches (LDA will give us both words and topics as a summary index). The modeling assumptions will be different in that case -- for instance, model word probability as uniform across party lines. Instead of building party differences into the structural model, we can then run a reduced form regression of y_i on party and other confounds. This would allow us to control for a bunch of other covariates that correlate with party and speech but do not suggest partisanship (geography is one example). The approach in the paper has an additional benefit of joint modeling of language and partisanship, which helps with inference (y variable is estimated rather than given in the data).\n",
    "\n",
    "- Can't we use LASSO to select partisan phrases? What's the gain of specifying a behavior structural model here? \n",
    "\n",
    "A. This is a natural choice following the specification of a structural model. LASSO can be an easy alternative though. The gain is interpretability and joint estimation of model parameters that include covariates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb43940",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "source": [
    "### [HIDE THIS FROM STUDENTS]Homework question ideas for future\n",
    "NLP + ECONOMICS\n",
    "\n",
    "- use textual features in combination of meta data (e.g., categorical variables and other numeric variables) to predict\n",
    "- an example demonstrating that topic model does not always work better than rule-based approaches (think about measuring Economic policy uncertainty)\n",
    "- pipeline from retrieving data to working with it\n",
    "- theory questions on topic modeling?, text? bias? causal inference with text?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
